MapReduce is a programming paradigm that enables massive scalability across hundreds or thousands of servers in a Hadoop cluster. As the processing component, MapReduce is the heart of Apache Hadoop. The term "MapReduce" refers to two separate and distinct tasks that Hadoop programs perform. The first is the map job, which takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs).
The reduce job takes the output from a map as input and combines those data tuples into a smaller set of tuples. As the sequence of the name MapReduce implies, the reduce job is always performed after the map job.

MapReduce is composed of several components, including :
	•	JobTracker — The master node that manages all jobs and resources in a cluster
	•	TaskTrackers — Agents deployed to each machine in the cluster to run the map and reduce tasks
	•	JobHistoryServer — A component that tracks completed jobs, and is typically deployed as a separate function or with JobTracker
